str(train)
train[,"Order Number"] <- as.factor(train[,"Order Number"])
#Cleaning Data
str(train)
train[,"Order Number"] <- as.factor(train[,"Order Number"])
#Cleaning Data
str(train)
train[,"Product Price"]<- train[,"Product Price"]== NULL
#Cleaning Data
str(train)
train[,"Order Number"] <- as.factor(train[,"Order Number"])
train[,"Order Date"]<- train[,"Order Date"]== NULL
train[,"Product Price"]<- train[,"Product Price"]== NULL
train[,"Total products"]<- train[,"Total products"]== NULL
# Fitting model
# Training Apriori on the dataset
set.seed = 220 # Setting seed
associa_rules = apriori(data = train,
parameter = list(support = 0.004,
confidence = 0.2))
# Plot
itemFrequencyPlot(dataset, topN = 10)
# Visualising the results
inspect(sort(associa_rules, by = 'lift')[1:10])
plot(associa_rules, method = "graph",
measure = "confidence", shading = "lift")
#two types of formats = "basket" or "single" -> how to get from sql database w/o going through csv
data <- read.transactions("C:/Users/Loy/Documents/Beeware/TestData/restaurant-1-orders.csv",format="single",sep=',',cols=c(1,3)) #Cols 1 = Order no, col 2 = item name
inspect(head(data,3))
#########DATA Exploration############
library("ggplot2") #visualization
library("corrplot") #correlation plot
library("e1071") #skewness
library("dplyr") #manipulation
library("data.table") # reading data
library("dplyr")
library("tidyverse")
library(caTools)
library(randomForest)
library(arules)
library(arulesViz)
#two types of formats = "basket" or "single" -> how to get from sql database w/o going through csv
data <- read.transactions("C:/Users/Loy/Documents/Beeware/TestData/restaurant-1-orders.csv",format="single",sep=',',cols=c(1,3)) #Cols 1 = Order no, col 2 = item name
inspect(head(data,3))
# Plot
itemFrequencyPlot(data, topN = 10,type='relative')
head(itemFrequency(data)) #First 6 supports by alphabet
unformatted_data <- fread("C:/Users/Loy/Documents/Beeware/TestData/restaurant-1-orders.csv")
head(unformatted_data,5)
head(unformatted_data,10)
#two types of formats = "basket" or "single" -> how to get from sql database w/o going through csv
data <- read.transactions("C:/Users/Loy/Documents/Beeware/TestData/restaurant-1-orders.csv",format="single",sep=',',cols=c(1,3)) #Cols 1 = Order no, col 2 = item name
inspect(head(data,3))
head(unformatted_data[order(unformatted_data["order Number"]),],10)
final_data <- unformatted_data[order(unformatted_data[,"Order Number"])
head(final_data,10)
unformatted_data[,"Order Number"]
order(unformatted_data[,"Order Number"])
final_data <- unformatted_data[order(unformatted_data[,"Order Number"])]
head(final_data,10)
head(final_data,20)
#########DATA Exploration############
library("ggplot2") #visualization
library("corrplot") #correlation plot
library("e1071") #skewness
library("dplyr") #manipulation
library("data.table") # reading data
library("dplyr")
library("tidyverse")
library(caTools)
library(randomForest)
library(arules)
library(arulesViz)
unformatted_data <- fread("C:/Users/Loy/Documents/Beeware/TestData/restaurant-1-orders.csv")
final_data <- unformatted_data[order(unformatted_data[,"Order Number"])]
head(final_data,20)
#two types of formats = "basket" or "single" -> how to get from sql database w/o going through csv
data <- read.transactions("C:/Users/Loy/Documents/Beeware/TestData/restaurant-1-orders.csv",format="single",sep=',',cols=c(1,3)) #Cols 1 = Order no, col 2 = item name
inspect(head(data,3))
# Plot
itemFrequencyPlot(data, topN = 10,type='relative')
head(itemFrequency(data)) #First 6 supports by alphabet
itemFrequency(data)['Garlic Naan']
length(data)
itemFrequency(data)[grep("Naan",itemLabels(data))] #Find all items with Naan and their support
# Fitting model
# Training Apriori on the dataset
set.seed = 220 # Setting seed
associa_rules = apriori(data = data,
parameter = list(support = 0.004, #support of 0.004 = at least 0.4% of rows need to contain it = 13398rows *0.001=54
confidence = 0.2, #confidence of 0.2 = at least 20% of the instances of the rule need to be fulfilled positively =  0.2 *54rows=10.8
minlen=2, #one in if and one in then
maxlen=4))
#good options for support = 0.001 and confidence = 0.8
# Visualising the results
inspect(sort(associa_rules, by = 'lift')[1:10]) #inspect by highest lift first, lift = how many more times rhs is purchased if lhs is in cart
inspect(sort(associa_rules, by = 'confidence')[1:10]) #inspect by highest confidence first
inspect(sort(associa_rules, by = 'support')[1:10]) #inspect by highest support first
plot(associa_rules, method = "graph",
measure = "confidence", shading = "lift")
#simple rules
simple_rules <- apriori(data = data,
parameter = list(support = 0.004,
confidence = 0.8,
minlen=2,
maxlen=3))
length(simple_rules)
impt_rules<-sort(simple_rules, by = 'lift')[1:10]
length(impt_rules)
plot(impt_rules,method="graph",engine="htmlwidget")
#Filter rules
rules.sub <- subset(rules, subset = lhs %in% "Mint Sauce" & lift > 2)
#Filter rules
rules.sub <- subset(associa_rules, subset = lhs %in% "Mint Sauce" & lift > 2)
inspect(rules.sub)
inspect(sort(rules.sub,by ='lift'))
# LHS contains only Mint Sauce
rules.sub <- subset(associa_rules, subset = lhs %pin% "Mint Sauce" & lift > 2)
inspect(sort(rules.sub,by ='lift'))
inspect(sort(rules.sub,by ='lift')[1:10])
# LHS contains only Mint Sauce
rules.sub <- subset(associa_rules, subset = lhs %ain% "Mint Sauce" & lift > 2)
inspect(sort(rules.sub,by ='lift')[1:10])
#Filter rules
# LHS contains Mint Sauce
rules.sub <- subset(associa_rules, subset = lhs %in% "Mint Sauce" & lift > 2 & confidence > 0.5)
inspect(sort(rules.sub,by ='lift')[1:10])
#Filter rules
# LHS contains Mint Sauce
rules.sub <- subset(associa_rules, subset = lhs %ain% "Mint Sauce" & lift > 2 & confidence > 0.5)
inspect(sort(rules.sub,by ='lift')[1:10])
unformatted_data <- fread("C:/Users/Loy/Documents/Beeware/TestData/restaurant-1-orders.csv")
final_data <- unformatted_data[order(unformatted_data[,"Order Number"])]
head(final_data,20)
head(final_data[,"Order Number"=630])
final_data[,"Order Number"=630]
final_data[,"Order Number"==630]
final_data["Order Number"==630]
#########DATA Exploration############
library("ggplot2") #visualization
library("corrplot") #correlation plot
library("e1071") #skewness
library("dplyr") #manipulation
library("data.table") # reading data
library("dplyr")
library("tidyverse")
library(caTools)
library(randomForest)
library(arules)
library(arulesViz)
unformatted_data <- fread("C:/Users/Loy/Documents/Beeware/TestData/restaurant-1-orders.csv")
final_data <- unformatted_data[order(unformatted_data[,"Order Number"])]
head(final_data)
#two types of formats = "basket" or "single" -> how to get from sql database w/o going through csv
data <- read.transactions("C:/Users/Loy/Documents/Beeware/TestData/restaurant-1-orders.csv",format="single",sep=',',cols=c(1,3)) #Cols 1 = Order no, col 2 = item name
inspect(head(data,3))
# Plot
itemFrequencyPlot(data, topN = 10,type='relative')
head(itemFrequency(data)) #First 6 supports by alphabet
itemFrequency(data)['Garlic Naan']
length(data)
itemFrequency(data)[grep("Naan",itemLabels(data))] #Find all items with Naan and their support
# Fitting model
# Training Apriori on the dataset
set.seed = 220 # Setting seed
associa_rules = apriori(data = data,
parameter = list(support = 0.004, #support of 0.004 = at least 0.4% of rows need to contain it = 13398rows *0.001=54
confidence = 0.2, #confidence of 0.2 = at least 20% of the instances of the rule need to be fulfilled positively =  0.2 *54rows=10.8
minlen=2, #one in if and one in then
maxlen=4))
#good options for support = 0.001 and confidence = 0.8
# Visualising the results
inspect(sort(associa_rules, by = 'lift')[1:10]) #inspect by highest lift first, lift = how many more times rhs is purchased if lhs is in cart
inspect(sort(associa_rules, by = 'confidence')[1:10]) #inspect by highest confidence first
inspect(sort(associa_rules, by = 'support')[1:10]) #inspect by highest support first
plot(associa_rules, method = "graph",
measure = "confidence", shading = "lift")
#simple rules
simple_rules <- apriori(data = data,
parameter = list(support = 0.004,
confidence = 0.8,
minlen=2,
maxlen=3))
length(simple_rules)
impt_rules<-sort(simple_rules, by = 'lift')[1:10]
length(impt_rules)
plot(impt_rules,method="graph",engine="htmlwidget")
#Filter rules
# LHS contains Mint Sauce
rules.sub <- subset(associa_rules, subset = lhs %ain% "Mint Sauce" & lift > 2 & confidence > 0.5)
inspect(sort(rules.sub,by ='lift')[1:10])
# Plot
itemFrequencyPlot(data, topN = 10,type='relative')
head(itemFrequency(data)) #First 6 supports by alphabet
itemFrequency(data)['Garlic Naan']
length(data)
itemFrequency(data)[grep("Naan",itemLabels(data))] #Find all items with Naan and their support
summary(data)
plot(associa_rules, method = "graph",
measure = "confidence", shading = "lift")
plot(associa_rules)
head(quality(rules))
head(quality(associa_rules))
plot(associa_rules, method = "graph",
measure = c("confidence","lift"), shading = "confidence")
plot(associa_rules,
measure = c("confidence","lift"), shading = "confidence")
plot(associa_rules,
measure = c("support","lift"), shading = "confidence")
#Invoke interactive Plot
sel <- plot(associa_rules,
measures=c("support","lift")
shading="confidence",
interactive=TRUE)
sel <- plot(associa_rules,
measures=c("support","lift")
shading="confidence",
interactive=TRUE)
#Invoke interactive Plot
sel <- plot(associa_rules,
measures=c("support","lift"),
shading="confidence",
interactive=TRUE)
#Loading Dependencies
library(data.table)
library(tidyverse)
library(caTools)
library(rpart)
library(rpart.plot)
setwd("C:/Users/Loy/Documents")
data.dt <- fread("1979-2021.csv")
head(data.dt,20)
usd.dt <- data.dt[1:2]
View(data.dt)
head(usd.dt,20)
usd.dt <- data.dt[,1:2]
head(usd.dt,20)
usd.dt[,Change:= United States(USD)-shift(United States(USD))]
usd.dt[, mom_change:= United States(USD)-shift(United States(USD))]
usd.dt[, mom_change:= "United States(USD)"-shift("United States(USD)")]
usd.dt<- usd.dt  %>% rename(United States(USD) = USD)
usd.dt<- usd.dt  %>% rename(usd.dt["United States(USD)"] = USD)
names(usd.dt)[2]<-"USD"
head(usd.dt,20)
usd.dt[, mom_change:=USD -shift(USD)]
head(usd.dt,20)
usd.dt[, mom_change:= (USD-shift(USD))/USD]
head(usd.dt,20)
usd.dt[, mom_change:= (USD-shift(USD))*100/USD]
head(usd.dt,20)
type(usd.dt$Date)
summary(usd.dt$Date)
?shift
usd.dt[,avg := (mom_change+shift(mom_change,n=12,type="lead"))/2]
head(usd.dt,20)
head(usd.dt,20,desc)
tail(usd.dt)
tail(usd.dt,20)
tail(usd.dt,30)
tail(usd.dt,31)
1979-202100000000000
#########DATA Exploration############
library("ggplot2") #visualization
library("corrplot") #correlation plot
library("e1071") #skewness
library("dplyr") #manipulation
library("data.table") # reading data
library("dplyr")
library("tidyverse")
library(caTools)
library(randomForest)
library(arules)
library(arulesViz)
unformatted_data <- fread("C:/Users/Loy/Documents/Beeware/TestData/restaurant-1-orders.csv")
final_data <- unformatted_data[order(unformatted_data[,"Order Number"])]
head(final_data)
#two types of formats = "basket" or "single" -> how to get from sql database w/o going through csv
data <- read.transactions("C:/Users/Loy/Documents/Beeware/TestData/restaurant-1-orders.csv",format="single",sep=',',cols=c(1,3)) #Cols 1 = Order no, col 2 = item name
inspect(head(data,3))
# Plot
itemFrequencyPlot(data, topN = 10,type='relative')
head(itemFrequency(data)) #First 6 supports by alphabet
itemFrequency(data)['Garlic Naan']
length(data)
itemFrequency(data)[grep("Naan",itemLabels(data))] #Find all items with Naan and their support
summary(data)
# Fitting model
# Training Apriori on the dataset
set.seed = 220 # Setting seed
associa_rules = apriori(data = data,
parameter = list(support = 0.004, #support of 0.004 = at least 0.4% of rows need to contain it = 13398rows *0.001=54
confidence = 0.2, #confidence of 0.2 = at least 20% of the instances of the rule need to be fulfilled positively =  0.2 *54rows=10.8
minlen=2, #one in if and one in then
maxlen=4))
#good options for support = 0.001 and confidence = 0.8
# Visualising the results
inspect(sort(associa_rules, by = 'lift')[1:10]) #inspect by highest lift first, lift = how many more times rhs is purchased if lhs is in cart
inspect(sort(associa_rules, by = 'confidence')[1:10]) #inspect by highest confidence first
inspect(sort(associa_rules, by = 'support')[1:10]) #inspect by highest support first
plot(associa_rules)
head(quality(associa_rules))
plot(associa_rules, method = "graph",
measure = "confidence", shading = "lift")
plot(associa_rules,
measure = c("support","lift"), shading = "confidence")
#simple rules
simple_rules <- apriori(data = data,
parameter = list(support = 0.004,
confidence = 0.8,
minlen=2,
maxlen=3))
length(simple_rules)
impt_rules<-sort(simple_rules, by = 'lift')[1:10]
length(impt_rules)
plot(impt_rules,method="graph",engine="htmlwidget")
#Filter rules
# LHS contains Mint Sauce
rules.sub <- subset(associa_rules, subset = lhs %ain% "Mint Sauce" & lift > 2 & confidence > 0.5)
inspect(sort(rules.sub,by ='lift')[1:10])
#Invoke interactive Plot
sel <- plot(associa_rules,
measures=c("support","lift"),
shading="confidence",
interactive=TRUE)
#Filter rules
# LHS contains Mint Sauce
rules.sub <- subset(associa_rules, subset = lhs %ain% "Mint Sauce" & lift > 2 & confidence > 0.5)
inspect(sort(rules.sub,by ='lift')[1:10])
rules.sub <- subset(associa_rules, subset = lhs %ain% "Mint Sauce" & lift > 2 & confidence > 0.5)
inspect(sort(rules.sub,by ='lift')[1:10])
inspect(sort(rules.sub,by ='lift')[1:10])
plot(impt_rules,method="graph",engine="htmlwidget")
rules.sub <- subset(associa_rules, subset = rhs %ain% "Mint Sauce" & lift > 2 & confidence > 0.5)
inspect(sort(rules.sub,by ='lift')[1:10])
impt_rules.groupby('antecedents').size().sort_values(ascending=False)
market_basket_rules = association_rules(impt_rules, metric="lift", min_threshold=1)
item_rec <- associa_rules.drop_duplicates
?subset
?sort
impt_rules<-sort(simple_rules, by = 'lift')[1:20]
length(impt_rules)
plot(impt_rules,method="graph",engine="htmlwidget")
impt_rules<-sort(simple_rules, by = 'lift')[1:20]
length(impt_rules)
plot(impt_rules,method="graph",engine="htmlwidget")
inspect(impt_rules)
#simple rules
simple_rules <- apriori(data = data,
parameter = list(support = 0.004,
confidence = 0.8,
minlen=2,
maxlen=5))
length(simple_rules)
impt_rules<-sort(simple_rules, by = 'lift')[1:20]
length(impt_rules)
plot(impt_rules,method="graph",engine="htmlwidget")
inspect(impt_rules)
#simple rules
simple_rules <- apriori(data = data,
parameter = list(support = 0.004,
confidence = 0.8,
minlen=2,
maxlen=3))
length(simple_rules)
impt_rules<-sort(simple_rules, by = 'lift')[1:20]
length(impt_rules)
plot(impt_rules,method="graph",engine="htmlwidget")
inspect(impt_rules)
top_20_frequence_items = sort(associa_rules,by = 'support',ascending=False).head(20)
top_20_frequence_items = inspect(sort(associa_rules,by = 'support',ascending=False),20)
top_20_frequence_items = inspect(sort(associa_rules,by = 'support',ascending=False))
top_20_frequence_items = inspect(sort(associa_rules,by = 'support',ascending=False)[1:20])
top_20_frequence_items.drop_duplicates(subset=['lhs'])
?subset
?drop_duplicates
?drop_duplicate
?DISTINCT
?distinct
distinct(top_20_frequence_items)
distinct(top_20_frequence_items,lhs)
distinct(top_20_frequence_items,.keep_all=TRUE)
top_20_frequence_items %>% distinct(lhs,.keep_all=TRUE)
top_20_frequence_items %>% distinct()
type(top_20_frequence_items)
typeof(top_20_frequence_items)
top_20_frequence_items %>% unique(lhs,.keep_all=TRUE)
top_20_frequence_items %>% unique(lhs)
?unique
top_20_frequence_items %>% unique(lhs,incomparables = FALSE,)
unique(top_20_frequence_items)
unique(top_20_frequence_items[lhs])
top_20_frequence_items[lhs]
#simple rules
simple_rules <- apriori(data = data,
parameter = list(support = 0.004,
confidence = 0.8,
minlen=2,
maxlen=3))
length(simple_rules)
top_20_frequence_items = inspect(sort(simple_rules,by = 'support',ascending=False)[1:20])
top_20_frequence_items = inspect(sort(associa_rules,by = 'support',ascending=False)[1:20])
top_frequence_items = inspect(sort(associa_rules,by = 'support',ascending=False))
top_frequence_items = inspect(sort(associa_rules,by = 'support',ascending=False))
unique(top_frequence_items)
top_frequence_items[1]
unique(top_frequence_items[1])
unique(top_frequence_items[1])[1:20]
?isin
top_frequence_items = inspect(sort(associa_rules,by = 'support',ascending=False))
unique(top_frequence_items[1])
top_frequence_items unique(top_frequence_items[1])
dis_lhs <- unique(top_frequence_items[1])
dis_lhs[1:20]
dis_lhs[,10]
dis_lhs[1,10]
dis_lhs[0,10]
typeof(dis_lhs)
dis_lhs[1]
dis_lhs[1]
dis_lhs[2]
top_frequence_items[1]
best_item_rec <- top_frequence_items[top_frequence_items[1].isin(dis_lhs)]
best_item_rec <- top_frequence_items[top_frequence_items[1] in (dis_lhs)]
best_item_rec <- top_frequence_items[isin(top_frequence_items[1],dis_lhs, ordered = TRUE)]
library(prob)
install.packages(prob)
install.packages("prob")
library(prob)
best_item_rec <- top_frequence_items[top_frequence_items[1] %in% dis_lhs]
best_item_rec
inspect(best_item_rec)[1:10])
inspect(best_item_rec[1:10])
inspect(best_item_rec)
best_item_rec
list(best_item_rec)
inspect(list(best_item_rec))
dis_lhs <- unique(top_frequence_items[1])
top_frequence_items[1] %in% dis_lhs
top_frequence_items[1]
typeof(top_frequence_items[1])
dis_lhs
top_frequence_items[1] %in% dis_lhs
as.data.frame(dis_lhs)
x<-as.data.frame(dis_lhs)
x
summary(x)
y <-as.data.frame(top_frequence_items[1])
summary(y)
x %in% y
x == y
top_frequence_items %<% filter(top_frequence_items[1] %in% dis_lhs)
top_frequence_items %>% filter(top_frequence_items[1] %in% dis_lhs)
top_frequence_items %>% filter(lhs %in% dis_lhs)
head(final_data)
inspect(head(data,3))
rules_subset <- subset(top_frequence_items, (lhs %in% unique(data$items)))
rules_subset <- subset(top_frequence_items, (lhs %in% unique(top_frequence_items[1])))
rules_subset
rules_subset <- subset(top_frequence_items, (lhs %in% unique(top_frequence_items)))
rhs(top_frequence_items)
top_frequence_items = inspect(sort(associa_rules,by = 'support',ascending=False))
rhs(top_frequence_items)
rules_subset <- subset(top_frequence_items, (lhs %in% unique(lhs)))
inspect(rules_subset)
rules_subset
rules_subset <- subset(top_frequence_items, (lhs %in% unique(lhs)))
rules_subset
rules_subset[1]
rules_subset[,1:10]
rules_subset[:,1:10]
rules_subset[,1:10]
rules_subset[1:5,1:10]
head(rules_subset,20)
top_frequence_items = inspect(sort(associa_rules,by = 'support',ascending=False))
?subset
rules_subset <- subset(top_frequence_items, (lhs %oin% unique(lhs)))
rules_subset <- subset(top_frequence_items, (lhs %ain% unique(lhs)))
rules_subset <- subset(top_frequence_items, (lhs %iin% unique(lhs)))
rules_subset <- subset(top_frequence_items, (lhs %in% unique(lhs)))
?subset
library(caTools)            # Train-Test Split
library(randomForest)       # Random Forest
setwd("C:/Users/Loy/Documents/GitHub/BC2407-Analytics-II-Project/Models")
data1 <- read.csv("C:/Users/Loy/Documents/GitHub/BC2407-Analytics-II-Project/Dataset/trafficAccident2020_cleaned.csv", stringsAsFactors = T)
sum(is.na(data1))
## verifies no missing va
cat_vars <- sapply(data1, is.factor)
# Change the Baseline Reference level for All Categorical Features.
for (var in names(data1)[cat_vars]) {
data1[[var]] <- factor(data1[[var]], levels = unique(data1[[var]]))
}
# Train-test split ---------------------------------------------------------
set.seed(1)
train <- sample.split(Y=data1$Fatality_Rate, SplitRatio = 0.7)
trainset <- subset(data1, train==T)
testset <- subset(data1, train==F)
#Trainset Error
# RF at default settings trained on 70% split train data
m.RF.test <- randomForest(Fatality_Rate ~ . , data = trainset,
na.action = na.omit,
importance = T)
m.RF.test
###Variable Importance on Trainset
var.impt <- importance(m.RF.1)
###Variable Importance on Trainset
var.impt <- importance(m.RF.test)
varImpPlot(m.RF.test, type = 1)
m.RF.test.yhat <- predict(m.RF.test, newdata = testset)
RMSE.test.mars2 <- round(sqrt(mean((testset$resale_price - m.mars2.yhat)^2)))
m.RF.test.yhat <- predict(m.RF.test, newdata = testset)
RMSE.test.mars2 <- round(sqrt(mean((testset$Fatality_Rate - m.RF.test.yhat)^2)))
RMSE.test.RF <- round(sqrt(mean((testset$Fatality_Rate - m.RF.test.yhat)^2)))
